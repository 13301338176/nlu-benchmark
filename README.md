# Natural Language Understanding benchmark

This repository contains the results of two benchmarks that compare natural language understanding services offering:
1. **built-in intents** (Apple’s SiriKit, Amazon’s Alexa, Microsoft’s Luis,
Google’s API.ai, and [Snips.ai](https://snips.ai/)) on a selection of
various intents. This benchmark was performed in December 2016. Its results
are described in length in the [following post](
https://snips.ai/content/sdk-benchmark-visualisation/).
2. **custom intent engines** (Google's API.ai, Facebook's Wit, Microsoft's Luis, Amazon's Alexa, and Snips' NLU) for seven chosen intents. This benchmark was performed in June 2017. Its results are described in the [following post](https://medium.com/@alicecoucke/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19).
3. **extension of Braun et al., 2017** (Google's API.AI, Microsoft's Luis, IBM's Watson, Rasa)
This experiment replicates the analysis made by Braun et al., 2017, published in Evaluating Natural Language Understanding Services for Conversational Question Answering Systems as part of SIGDIAL 2017 proceedings. Snips and Rasa are added.

The data is provided for each benchmark and more details about the methods are available in the README file in each folder.
